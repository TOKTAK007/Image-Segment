{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZAFKa3856WvO+wgZCNHpS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TOKTAK007/Image-Segment/blob/main/RunlotterywithRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVyW93zJmf4i",
        "outputId": "94d4dfbc-d017-4161-bbd4-56160ebafaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 6, 1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "dataset = np.array([\n",
        "    [984906, 87907, 25873, 417652, 590417, 297411],\n",
        "    [812519, 157196, 845093, 375805, 121789, 913106],\n",
        "    [613106, 484669, 943703, 929332, 331583, 436594],\n",
        "    [620405, 981417, 361807, 319196, 155012, 658642],\n",
        "    [395919, 970618, 737867, 61905, 98597, 944308],\n",
        "    [880159, 819068, 639235, 77258, 32761, 45037],\n",
        "    [386372, 578171, 70935, 114475, 46750, 910261],\n",
        "    [556725, 713517, 691861, 292972, 684579, 501272],\n",
        "    [100787, 472270, 890422, 835538, 424603, 912307],\n",
        "    [38495, 803628, 201303, 100994, 972661, 506404],\n",
        "    [286051, 837893, 244083, 999997, 945811, 569391],\n",
        "    [873286, 347258, 516967, 831567, 51095, 503446],\n",
        "    [875938, 781403, 589227, 491774, 510541, 529924],\n",
        "    [453522, 17223, 967375, 812564, 691197, 340388],\n",
        "    [798787, 775476, 387006, 369765, 943647, 174055],\n",
        "    [516461, 962526, 61324, 570331, 109767, 724628],\n",
        "    [345650, 74824, 967134, 197079, 735867, 356564]\n",
        "])\n",
        "\n",
        "# normalize the data to be between 0 and 1\n",
        "dataset = dataset / 999999\n",
        "\n",
        "# reshape the data to be 102 samples of 6 timesteps\n",
        "dataset = np.reshape(dataset, (dataset.shape[0], dataset.shape[1], 1))\n",
        "batch_size = dataset.shape[0]\n",
        "hidden_size = 32\n",
        "num_layers = 2\n",
        "print (dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        out, (h0, c0) = model.lstm(input_data, (h0, c0))  # Pass hx and cx to LSTM\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "-NLqI4bfmvhI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LSTM(input_size=1, hidden_size=32, num_layers=2, output_size=1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 1000"
      ],
      "metadata": {
        "id": "4s22UuFcm2B2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.tensor([[int(digit) for digit in row[:-1]] for row in dataset]).float()\n",
        "input_data = input_data.reshape(-1, 5, 1) # Reshape to (batch_size, sequence_length, input_size=1)\n",
        "\n",
        "output_data = torch.tensor([int(row[-1]) for row in dataset]).long()"
      ],
      "metadata": {
        "id": "hy0jlhk8up4_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiCfYLD3yN1W",
        "outputId": "afdd1ae3-1dfd-4353-f934-e0bc687a0802"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([17, 5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    outputs = model(input_data.int())\n",
        "    loss = criterion(outputs, output_data.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f550382-c07f-468f-fddf-5d5b4b03ea85",
        "id": "RYs1OUcxut1Q"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Loss: 0.0000\n",
            "Epoch 101/1000, Loss: 0.0000\n",
            "Epoch 201/1000, Loss: 0.0000\n",
            "Epoch 301/1000, Loss: 0.0000\n",
            "Epoch 401/1000, Loss: 0.0000\n",
            "Epoch 501/1000, Loss: 0.0000\n",
            "Epoch 601/1000, Loss: 0.0000\n",
            "Epoch 701/1000, Loss: 0.0000\n",
            "Epoch 801/1000, Loss: 0.0000\n",
            "Epoch 901/1000, Loss: 0.0000\n"
          ]
        }
      ]
    }
  ]
}