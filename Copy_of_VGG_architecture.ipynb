{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TOKTAK007/Image-Segment/blob/main/Copy_of_VGG_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGFMlprVNhM9",
        "outputId": "94c4a394-6122-4588-df95-045860413bc0"
      },
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms,models,datasets\n",
        "!pip install torch_summary\n",
        "from torchsummary import summary\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch_summary\n",
            "Successfully installed torch_summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchsummary เป็นเครื่องมือ (tool) สำหรับการสรุปโครงสร้างของโมเดลประสาทเทียม (neural network) ใน PyTorch ที่ช่วยให้ผู้ใช้สามารถตรวจสอบขนาดของแต่ละชั้น (layer) และจำนวนพารามิเตอร์ (parameters) ในโมเดลได้อย่างง่ายดาย โดยใช้ฟังก์ชัน summary ซึ่งเป็นฟังก์ชันที่สร้างขึ้นมาเพื่อให้ผู้ใช้สามารถสรุปโครงสร้างของโมเดลได้อย่างรวดเร็ว\n",
        "\n"
      ],
      "metadata": {
        "id": "6na0QCFJ0PJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision เป็นชุดเครื่องมือ (toolkit) สำหรับการประมวลผลภาพ (image processing) ใน PyTorch ซึ่งช่วยให้ผู้ใช้สามารถทำงานกับข้อมูลภาพได้อย่างสะดวกและรวดเร็วขึ้น ซึ่งได้รวมถึงการโหลด (loading) และการประมวลผลภาพต่างๆ เช่น การแปลงขนาด (resizing), การทำซ้อนภาพ (image overlay), การแสดงผล (visualization) และอื่นๆ อีกมากมาย นอกจากนี้ torchvision ยังมีโมดูลสำหรับการประมวลผลข้อมูลวิดีโอ (video processing) อีกด้วย"
      ],
      "metadata": {
        "id": "hEXgoAS5zdue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.nn.functional เป็นโมดูลของ PyTorch ที่มีฟังก์ชัน (function) ต่างๆ ที่ใช้ในการประมวลผลข้อมูลในโมเดลประสาทเทียม (neural network) เช่น การคำนวณค่าเสีย (loss), การประมวลผลทางคณิตศาสตร์ (mathematical operations) และการประมวลผลข้อมูลในชั้นต่างๆ ของโมเดลประสาทเทียม เช่น การทำคอนโวลูชัน (convolution), การทำประสาทเทียมแบบลูกถ้วย (LSTM), การทำเอฟเฟกต์ (effect) และอื่นๆ อีกมากมาย\n",
        "\n",
        "การ import torch.nn.functional as F จึงเป็นการ import โมดูล functional ของ torch.nn และกำหนดชื่อตัวแปรให้เป็น F เพื่อที่จะเข้าถึงฟังก์ชันต่างๆ ในโมดูลนี้ได้อย่างสะดวก โดยจะใช้ F แทน torch.nn.functional ในการเรียกใช้ฟังก์ชันต่างๆ ที่อยู่ใน functional นั้น\n"
      ],
      "metadata": {
        "id": "zmh0-r38zf1r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VRuWU7rNjNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6b4733-31eb-4c2a-81a1-31b86f638c7f"
      },
      "source": [
        "model = models.vgg16(pretrained=True).to(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 122MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "โค้ด model = models.vgg16(pretrained=True).to(device) ใช้โมเดล VGG-16 ที่ถูกสร้างด้วยไลบรารี Torchvision ใน PyTorch โดยกำหนดให้โมเดลนี้โหลดน้ำหนักก่อน (pretrained) จากชุดข้อมูล ImageNet และนำมาใช้ในการคำนวณผ่าน CPU/GPU ซึ่งเราสามารถระบุได้ด้วย device ที่กำหนดไว้\n",
        "\n",
        "โมเดล VGG-16 เป็นโมเดลประสาทเทียมที่มีความลึก 16 ชั้น (layers) และถูกออกแบบมาเพื่อใช้ในงานปัญญาประดิษฐ์ที่เกี่ยวข้องกับการจำแนกหมวดหมู่ภาพ โดยสามารถใช้งานได้ทั้งในการทำ Transfer Learning และ Fine-tuning กับชุดข้อมูลภาพต่าง ๆ ที่มีความซับซ้อนต่างกัน\n",
        "\n",
        "โค้ด to(device) จะนำโมเดล VGG-16 มาเรียกใช้บน CPU/GPU ที่กำหนดไว้ด้วย device ซึ่งทำให้เราสามารถใช้งานโมเดลได้เร็วขึ้นและสามารถปรับใช้กับข้อมูลที่มีขนาดใหญ่ได้มากขึ้น โดยที่ไม่เกิดปัญหาเกี่ยวกับ Memory ขณะที่ทำงานกับชุดข้อมูลที่มีขนาดใหญ่\n",
        "\n",
        "====================================="
      ],
      "metadata": {
        "id": "n1JTePz01OSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "โดยในตัวอย่างข้างต้น torch.zeros((3, 4)) จะสร้าง Tensor 2 มิติที่มีค่าเป็น 0 ทั้งหมด ขนาด 3x4 และ torch.zeros((2, 3, 4), dtype=torch.int) จะสร้าง Tensor 3 มิติที่มีค่าเป็น 0 ทั้งหมด ขนาด 2x3x4 "
      ],
      "metadata": {
        "id": "zDffZdff2qrA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ9BIH_QNkyY",
        "outputId": "1531811d-e5f7-4ffd-8fcd-33df1cfc22c8"
      },
      "source": [
        "summary(model, torch.zeros(1,3,224,224));"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
            "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        1,792\n",
            "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
            "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        36,928\n",
            "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
            "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
            "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       73,856\n",
            "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
            "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       147,584\n",
            "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
            "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
            "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         295,168\n",
            "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
            "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         590,080\n",
            "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
            "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         590,080\n",
            "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
            "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
            "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         1,180,160\n",
            "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
            "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         2,359,808\n",
            "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
            "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         2,359,808\n",
            "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
            "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         2,359,808\n",
            "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         2,359,808\n",
            "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         2,359,808\n",
            "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
            "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
            "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 7, 7]           --\n",
            "├─Sequential: 1-3                        [-1, 1000]                --\n",
            "|    └─Linear: 2-32                      [-1, 4096]                102,764,544\n",
            "|    └─ReLU: 2-33                        [-1, 4096]                --\n",
            "|    └─Dropout: 2-34                     [-1, 4096]                --\n",
            "|    └─Linear: 2-35                      [-1, 4096]                16,781,312\n",
            "|    └─ReLU: 2-36                        [-1, 4096]                --\n",
            "|    └─Dropout: 2-37                     [-1, 4096]                --\n",
            "|    └─Linear: 2-38                      [-1, 1000]                4,097,000\n",
            "==========================================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 15.61\n",
            "==========================================================================================\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 103.43\n",
            "Params size (MB): 527.79\n",
            "Estimated Total Size (MB): 631.80\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "summary(model, torch.zeros(1,3,224,224)) เป็นการเรียกฟังก์ชัน summary ซึ่งเป็นฟังก์ชันที่ช่วยสรุปข้อมูลของโมเดลประสาทเทียมและจำนวนพารามิเตอร์ทั้งหมดในโมเดล โดยรับอินพุตเป็นโมเดลประสาทเทียม (model) และตัวอย่างข้อมูล (input) ที่ต้องการส่งเข้าไปในโมเดลเพื่อคำนวณผลลัพธ์\n",
        "\n",
        "ในที่นี้ model คือโมเดล VGG-16 ที่ถูกสร้างขึ้นด้วยไลบรารี Torchvision และ torch.zeros(1,3,224,224) คือตัวอย่างข้อมูลภาพที่มีขนาด 1 ชุด (batch size) และมีขนาดภาพเป็น 3x224x224 (ความสูง 224 พิกเซล ความกว้าง 224 พิกเซล และมีช่องสี RGB 3 ช่อง)\n",
        "\n",
        "ผลลัพธ์ที่ได้จากการเรียกใช้ summary(model, torch.zeros(1,3,224,224)) คือสรุปข้อมูลของโมเดล VGG-16 ที่เราสร้างขึ้น ได้แก่จำนวนชั้น (layers) ทั้งหมด, จำนวนพารามิเตอร์ (parameters) ทั้งหมด, และขนาดของ Tensor ที่เป็น Output ของแต่ละชั้น (Output Shape) เพื่อช่วยในการตรวจสอบและปรับแต่งโมเดลให้เหมาะสมกับงานที่ต้องการใช้งาน"
      ],
      "metadata": {
        "id": "-45qJV8w2Fhw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StQBD486NmM-",
        "outputId": "5377e72f-fb56-4120-a942-0e2255286120"
      },
      "source": [
        "model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2d: 2-1 [-1, 64, 224, 224] คือ output shape หรือขนาดของ Tensor ที่ผ่านการผ่าน Convolutional Layer แบบ 2 มิติ (2D Convolution) ด้วย filter หรือ kernel ที่มีขนาดเท่ากับ 3x3 และมี stride เท่ากับ 1 (คือขนาดของ stride คือ 1x1) โดย Input Tensor จะมีขนาดเท่ากับ [-1, 3, 224, 224] และมี batch size ไม่กำหนด (เป็น -1) ซึ่งในที่นี้จะถูกส่งเข้าไปในโมเดล CNN แบบ VGG16 ที่มี pretrained weights โดย output Tensor จะมีขนาด 64 ช่อง (channel) และขนาดของภาพเท่ากับ 224x224 พร้อมกับ batch size เดียวกับ Input Tensor โดย [-1, 64, 224, 224] จะหมายความว่า batch size ยังคงเป็นตัวเดียวกับ Input Tensor ซึ่งจะนำไปผ่าน layer ต่อไปในโมเดล VGG16 ได้เรื่อยๆ จนกระทั่งเอาไปใช้ในการทำนายหรือในการเรียนรู้แบบ Supervised Learning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3PJSMnQm3zq_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBnhG2vTNr8R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}